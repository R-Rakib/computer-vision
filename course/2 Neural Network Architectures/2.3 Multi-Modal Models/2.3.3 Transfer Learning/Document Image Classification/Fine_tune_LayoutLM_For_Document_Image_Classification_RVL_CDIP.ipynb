{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybIvpxZAvM1V"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Tuha3PWqvUicwhS7uUGKH0HBI185pDY-?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDfla5EmvPp1"
      },
      "source": [
        "# Fine tuning LayoutLM for Document Image Classification Task\n",
        "\n",
        "In this notebook, we're going to go through fine-tuning LayoutLM Multimodal Model for Document Image Classification task with RVL-CDIP dataset\n",
        "\n",
        "- Small dataset at [here](https://huggingface.co/datasets/sitloboi2012/rvl_cdip_small_dataset), if you want to test with larger dataset, link [here](https://huggingface.co/datasets/sitloboi2012/rvl_cdip_large_dataset)\n",
        "- Model hub at [here](https://huggingface.co/docs/transformers/model_doc/layoutlm)\n",
        "- Academic paper for LayoutLM at [here](https://huggingface.co/docs/transformers/model_doc/layoutlm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6GRLy1pXtI"
      },
      "source": [
        "# Setting the environment\n",
        "\n",
        "- Use the `requirements.txt` file if you need to install `torch` and `torchvision` from the beginning. Else you can just run the second cell to install `transformers` and other relevant packages.\n",
        "\n",
        "- We also need to install `tesseract-ocr`.\n",
        "  - For Ubuntu user, you can use `sudo apt-get install tesseract-ocr`.\n",
        "  - For Windows user, please refer to this link\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc5pgDZdu49L"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "--find-links https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "transformers==4.33.1\n",
        "datasets\n",
        "seqeval\n",
        "torch==2.0.1+cu117\n",
        "torchvision==0.15.2+cu117\n",
        "pytesseract==0.3.10\n",
        "accelerate\n",
        "transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd7GkHVxwKd6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets seqeval pytesseract accelerate -q -q -q --exists-action i\n",
        "!sudo apt-get install tesseract-ocr\n",
        "\n",
        "# in case you need to install torch and torch vision\n",
        "# then uncomment and run the bottom pip install instead of the first one\n",
        "\n",
        "#pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKSyD7oc5YDy"
      },
      "outputs": [],
      "source": [
        "# login to huggingface hub to push the model at the end and pull the dataset down\n",
        "\n",
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPIk4zKAqUxa"
      },
      "source": [
        "# Get the dataset and preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJpdd1aHqlLD"
      },
      "source": [
        "We going to use `sitloboi2012/rvl_cdip_small_dataset` to run this. But you can also tag from `nielsr/rvl_cdip_10_examples_per_class`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nZsaO_X557kq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"sitloboi2012/rvl_cdip_small_dataset\")\n",
        "train_dataset = dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXaGCRkQtln3"
      },
      "source": [
        "Visualize the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV5U3L2vf_tm"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "image = train_dataset[\"image\"][0]\n",
        "image = image.convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyYngRR2toNf"
      },
      "source": [
        "Visualize the OCR using PyTesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9jGiuCFgIgw"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import numpy as np\n",
        "\n",
        "ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
        "ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "float_cols = ocr_df.select_dtypes('float').columns\n",
        "ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "words = ' '.join([word for word in ocr_df.text if str(word) != 'nan'])\n",
        "print(\"Word after run PyTesseract: \", words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30wAX9n2trql"
      },
      "source": [
        "Draw OCR Bounding Boxes on Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iPgQg1tgMUQ"
      },
      "outputs": [],
      "source": [
        "coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
        "actual_boxes = []\n",
        "for idx, row in coordinates.iterrows():\n",
        "    x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
        "    actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+width, top+height) to get the actual box\n",
        "    actual_boxes.append(actual_box)\n",
        "\n",
        "draw = ImageDraw.Draw(image, \"RGB\")\n",
        "for box in actual_boxes:\n",
        "  draw.rectangle(box, outline='red')\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wilvs8AytwFu"
      },
      "source": [
        "# Preprocess Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3Jh_YdhMgOov"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import datasets\n",
        "\n",
        "def normalize_box(box, width, height):\n",
        "     return [\n",
        "         int(1000 * (box[0] / width)),\n",
        "         int(1000 * (box[1] / height)),\n",
        "         int(1000 * (box[2] / width)),\n",
        "         int(1000 * (box[3] / height)),\n",
        "     ]\n",
        "\n",
        "def apply_ocr(row: datasets.formatting.formatting.LazyRow):\n",
        "    \"\"\"\n",
        "    Apply OCR PyTesseract to create a column for word token and bounding box location\n",
        "\n",
        "    Args:\n",
        "      row `(datasets.formatting.formatting.LazyRow)`: a row in the dataset\n",
        "\n",
        "    Returns:\n",
        "      `(datasets.formatting.formatting.LazyRow)`: updated row contains word token and bounding box\n",
        "    \"\"\"\n",
        "\n",
        "    # get the image\n",
        "    image = row[\"image\"]\n",
        "\n",
        "    width, height = image.size\n",
        "\n",
        "    # apply ocr to the image\n",
        "    ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
        "    float_cols = ocr_df.select_dtypes('float').columns\n",
        "    ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "    ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "    ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "    ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # get the words and actual (unnormalized) bounding boxes\n",
        "    #words = [word for word in ocr_df.text if str(word) != 'nan'])\n",
        "    words = list(ocr_df.text)\n",
        "    words = [str(w) for w in words]\n",
        "    coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
        "    actual_boxes = []\n",
        "    for idx, bbox_row in coordinates.iterrows():\n",
        "        x, y, w, h = tuple(bbox_row) # the row comes in (left, top, width, height) format\n",
        "        actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+width, top+height) to get the actual box\n",
        "        actual_boxes.append(actual_box)\n",
        "\n",
        "    # normalize the bounding boxes\n",
        "    boxes = []\n",
        "    for box in actual_boxes:\n",
        "        boxes.append(normalize_box(box, width, height))\n",
        "\n",
        "    # add as extra columns\n",
        "    assert len(words) == len(boxes)\n",
        "    row['words'] = words\n",
        "    row['bbox'] = boxes\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE9__VJSgbJh"
      },
      "outputs": [],
      "source": [
        "updated_dataset = train_dataset.map(apply_ocr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbRsRSjJt56o"
      },
      "source": [
        "# Processing Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT0T-XHhulnR"
      },
      "source": [
        "Convert label to list and to dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y56BAhnMQ0-b"
      },
      "outputs": [],
      "source": [
        "labels: list[str] = list(set(train_dataset[\"label\"]))\n",
        "id2label: dict[int, str] = dict(enumerate(labels))\n",
        "label2id: dict[str, int] = {k: v for v, k in enumerate(labels)}\n",
        "print(f\"Label to Id: {label2id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6AkWNouwsC"
      },
      "source": [
        "Initialize `Tokenizer` with pretrained and apply encode to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RDWCplDzgeWn"
      },
      "outputs": [],
      "source": [
        "from transformers import LayoutLMTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
        "\n",
        "def encode_example(example, max_seq_length=512, pad_token_box=[0, 0, 0, 0]):\n",
        "    \"\"\"\n",
        "    Get a row in the `dataset.Dataset` object and convert to correct input for the model\n",
        "    using the `Tokenizer`\n",
        "\n",
        "    Args:\n",
        "      row `(datasets.formatting.formatting.LazyRow)`: a row in the dataset object\n",
        "      max_seq_length `(int)`: the maximum sequence length to tokenize\n",
        "      pad_token_box `(list[int])`: the padding token\n",
        "\n",
        "    Returns:\n",
        "      `(datasets.formatting.formatting.LazyRow)`: the updated row contains the input shape for the model\n",
        "    \"\"\"\n",
        "    words = example['words']\n",
        "    normalized_word_boxes = example['bbox']\n",
        "\n",
        "    assert len(words) == len(normalized_word_boxes)\n",
        "\n",
        "    token_boxes = []\n",
        "    for word, box in zip(words, normalized_word_boxes):\n",
        "        word_tokens = tokenizer.tokenize(word)\n",
        "        token_boxes.extend([box] * len(word_tokens))\n",
        "\n",
        "    # Truncation of token_boxes\n",
        "    special_tokens_count = 2\n",
        "    if len(token_boxes) > max_seq_length - special_tokens_count:\n",
        "        token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n",
        "\n",
        "    # add bounding boxes of cls + sep tokens\n",
        "    token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]\n",
        "\n",
        "    encoding = tokenizer(' '.join(words), padding='max_length', truncation=True)\n",
        "    # Padding of token_boxes up the bounding boxes to the sequence length.\n",
        "    input_ids = tokenizer(' '.join(words), truncation=True)[\"input_ids\"]\n",
        "    padding_length = max_seq_length - len(input_ids)\n",
        "    token_boxes += [pad_token_box] * padding_length\n",
        "    encoding['bbox'] = token_boxes\n",
        "    encoding['label'] = label2id[example['label']]\n",
        "\n",
        "    assert len(encoding['input_ids']) == max_seq_length\n",
        "    assert len(encoding['attention_mask']) == max_seq_length\n",
        "    assert len(encoding['token_type_ids']) == max_seq_length\n",
        "    assert len(encoding['bbox']) == max_seq_length\n",
        "\n",
        "    return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVFmKz5ogqu6"
      },
      "outputs": [],
      "source": [
        "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Image\n",
        "\n",
        "# we need to define the features ourselves as the bbox of LayoutLM are an extra feature\n",
        "features = Features({\n",
        "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
        "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
        "    'attention_mask': Sequence(Value(dtype='int64')),\n",
        "    'token_type_ids': Sequence(Value(dtype='int64')),\n",
        "    'label': ClassLabel(names=labels),\n",
        "    'image': Image(),\n",
        "    'words': Sequence(feature=Value(dtype='string')),\n",
        "})\n",
        "\n",
        "encoded_dataset = updated_dataset.map(lambda example: encode_example(example), features=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jVGaJk6YhfW1"
      },
      "outputs": [],
      "source": [
        "encoded_dataset.set_format(type='torch', columns=['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWgUPIR-u5NS"
      },
      "source": [
        "Double check the data before feed to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ketJh6E1hgEF"
      },
      "outputs": [],
      "source": [
        "dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=1, shuffle=True)\n",
        "batch = next(iter(dataloader))\n",
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPIfy9GahjMx"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(batch['input_ids'][0].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljcs2a5XhlqR"
      },
      "outputs": [],
      "source": [
        "id2label[batch['label'][0].item()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDvapyvDu8W-"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHIp_rUavEQT"
      },
      "source": [
        "## Model Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfxBY2ExjYoQ"
      },
      "outputs": [],
      "source": [
        "from transformers import LayoutLMForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=len(labels))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_OfU0KIijiO-"
      },
      "outputs": [],
      "source": [
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yGJ1zFrvGaZ"
      },
      "source": [
        "## Metric Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99TzW5UXmeGB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Evaluate the model output base on multiple metrics\n",
        "\n",
        "    Args:\n",
        "      eval_pred `(Model.Output)`: the output of the model contains logits, and labels\n",
        "\n",
        "    Returns:\n",
        "      `dict[str, float]`: the evaluation result after calculate\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(predictions, labels)\n",
        "    precision_micro = precision_score(predictions, labels, average = \"micro\", zero_division=0)\n",
        "    precision_macro = precision_score(predictions, labels, average = \"macro\", zero_division=0)\n",
        "    recall_micro = recall_score(predictions, labels, average = \"micro\", zero_division=0)\n",
        "    recall_macro = recall_score(predictions, labels, average = \"macro\", zero_division=0)\n",
        "    f1_micro = f1_score(predictions, labels, average = \"micro\", zero_division=0)\n",
        "    f1_macro = f1_score(predictions, labels, average = \"macro\", zero_division=0)\n",
        "    return {\"accuracy\": accuracy, \"precision_micro\": precision_micro, \"recall_micro\": recall_micro, \"f1_micro\": f1_micro, \"precision_macro\": precision_macro, \"recall_macro\": recall_macro, \"f1_macro\": f1_macro}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws4ZUiadvMFz"
      },
      "source": [
        "## Training Arguments and Trainer Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uhCIMLNmj__0"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "metric_name = \"precision_micro\" # change if you want to\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir = \"result_trainer\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=10,\n",
        "    warmup_ratio = 0.1,\n",
        "    fp16 = True,\n",
        "    learning_rate = 1e-5,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    remove_unused_columns=False,\n",
        "    #push_to_hub = True,\n",
        "    #push_to_hub_model_id = f\"layoutlm-finetune-rvlcdip-small\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RaKXVZuwmnT9"
      },
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "early_stopping = EarlyStoppingCallback(early_stopping_patience = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "30Ex37d6nyOa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    train_args,\n",
        "    train_dataset=encoded_dataset,\n",
        "    eval_dataset=encoded_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBH-UpmBvSqW"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgzNUsFjoFAw"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImLEtkgPvT-3"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qigXvd9YqF5R"
      },
      "outputs": [],
      "source": [
        "predictions, labels, metrics = trainer.predict(encoded_dataset)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_NoSU8Jq8W4"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aae6z21mzPF1"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRVAltpbq-FP"
      },
      "outputs": [],
      "source": [
        "image = train_dataset[\"image\"][1].convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CNvZLfo9rHpE"
      },
      "outputs": [],
      "source": [
        "def inference_ocr(image):\n",
        "    \"\"\"\n",
        "    Copy version of the `apply_ocr` function but adapt for inference version\n",
        "\n",
        "    Args:\n",
        "      image (PIL.Image): the image to do OCR on\n",
        "\n",
        "    Returns:\n",
        "      `tuple[list[str], list[int]]`: contains the word token level and the bounding box respectively\n",
        "    \"\"\"\n",
        "    width, height = image.size\n",
        "\n",
        "    # apply ocr to the image\n",
        "    ocr_df = pytesseract.image_to_data(image, output_type='data.frame')\n",
        "    float_cols = ocr_df.select_dtypes('float').columns\n",
        "    ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "    ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
        "    ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "    ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
        "\n",
        "    # get the words and actual (unnormalized) bounding boxes\n",
        "    words = list(ocr_df.text)\n",
        "    words = [str(w) for w in words]\n",
        "    coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
        "    actual_boxes = []\n",
        "    for idx, row in coordinates.iterrows():\n",
        "        x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
        "        actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+width, top+height) to get the actual box\n",
        "        actual_boxes.append(actual_box)\n",
        "\n",
        "    # normalize the bounding boxes\n",
        "    boxes = [normalize_box(box, width, height) for box in actual_boxes]\n",
        "\n",
        "    # add as extra columns\n",
        "    assert len(words) == len(boxes)\n",
        "    return words, boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji1dW6YHrtUU"
      },
      "outputs": [],
      "source": [
        "word_ocr, boxes_ocr = inference_ocr(image)\n",
        "encoded_input = tokenizer(' '.join(word_ocr), padding='max_length', truncation=True, return_tensors = \"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "WLnKs-YaslCE"
      },
      "outputs": [],
      "source": [
        "for k,v in encoded_input.items():\n",
        "  encoded_input[k] = v.to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "j1RJmfJrszom"
      },
      "outputs": [],
      "source": [
        "outputs = model(**encoded_input)\n",
        "loss = outputs.loss\n",
        "logits = outputs.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vVALdvStOsu"
      },
      "outputs": [],
      "source": [
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGRj61IJzV0W"
      },
      "source": [
        "# Extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8J0UnDJzXHb"
      },
      "outputs": [],
      "source": [
        "# In case you want to load model from HuggingFace Hub\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\"<MODEL_TAG>\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzY-Krs0zf91",
        "outputId": "573769e1-951d-410f-b902-ee11d72f541e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using bos_token, but it is not set yet.\n",
            "Using eos_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "# In case you want to load model from checkpoint\n",
        "\n",
        "model = LayoutLMForSequenceClassification.from_pretrained(\"/content/result_trainer/checkpoint-20\")\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
